# Training Configuration
def train_model(model, train_loader, val_loader, num_epochs={{ config.training.epochs or 10 }}, learning_rate={{ config.training.learningRate or 0.001 }}, weight_decay={{ config.training.weightDecay or 0.0001 }}):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Define loss function
    {% if config.loss and config.loss.name %}
    criterion = {{ config.loss.name }}
    {% else %}
    criterion = nn.CrossEntropyLoss()
    {% endif %}

    # Define optimizer
    {% if config.optimizer and config.optimizer.name %}
    optimizer = {{ config.optimizer.name }}
    {% else %}
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    {% endif %}

    # Define learning rate scheduler
    {% if config.training.scheduler and config.training.scheduler != "None" %}
    scheduler = {{ config.training.scheduler }}(optimizer{% if config.training.schedulerParams %}, {% for k,v in config.training.schedulerParams.items() %}{{ k }}={{ v }}{% if not loop.last %}, {% endif %}{% endfor %}{% endif %})
    {% else %}
    scheduler = None
    {% endif %}

    {% if config.training.earlyStoppingEnabled %}
    # Early stopping parameters
    patience = {{ config.training.earlyStoppingPatience or 5 }}
    min_delta = {{ config.training.earlyStoppingMinDelta or 0.0001 }}
    counter = 0
    best_loss = float('inf')
    {% endif %}

    # Define metrics
    {% if config.metrics and config.metrics.name %}
    metric = {{ config.metrics.name }}
    {% else %}
    # Using loss for monitoring
    {% endif %}

    # Training loop
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
        for inputs, labels in train_pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            train_pbar.set_postfix({"loss": loss.item()})
        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_train_loss)

        model.eval()
        running_loss = 0.0
        val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]")
        with torch.no_grad():
            for inputs, labels in val_pbar:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                running_loss += loss.item() * inputs.size(0)
                val_pbar.set_postfix({"loss": loss.item()})
        epoch_val_loss = running_loss / len(val_loader.dataset)
        val_losses.append(epoch_val_loss)
        print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}")

        {% if config.training.scheduler and config.training.scheduler != "None" %}
        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(epoch_val_loss)
        else:
            scheduler.step()
        {% endif %}

        {% if config.training.earlyStoppingEnabled %}
        if epoch_val_loss < best_loss - min_delta:
            best_loss = epoch_val_loss
            counter = 0
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            counter += 1
            if counter >= patience:
                print(f"Early stopping triggered after {epoch+1} epochs")
                break
        {% endif %}

    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.savefig('loss_curve.png')
    plt.close()

    return model, train_losses, val_losses